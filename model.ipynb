{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import main\n",
    "key = '<replace with your key>' # this is the key for the weather data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this line on your own risk\n",
    "# it will take ~15-20 minutes to download the data and to do some cleaning\n",
    "#main(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import merged_cycle_data_file\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the weather & holiday data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import weather_data_csv\n",
    "\n",
    "holidays = 'holidays.csv'\n",
    "hol_df = pd.read_csv(holidays, index_col='date', parse_dates=['date'])\n",
    "weather_df = pd.read_csv(weather_data_csv, index_col='timestamp', parse_dates=['timestamp'])\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a set of the holiday dates\n",
    "hol_set= set(hol_df.index.map(lambda x: x.date()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the merged csv file by reading it in chunks\n",
    "\n",
    "# Warning:\n",
    "The next cell takes a lot of time (on one machine it took 3h 40 mins) so skip running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from datetime import datetime\n",
    "# import time\n",
    "# chunk_size = 1000000\n",
    "# date_mapper = lambda x: pd.to_datetime(datetime(year=x.year, month=x.month, day=x.day, hour=x.hour))\n",
    "\n",
    "# bike_share_df = pd.DataFrame()\n",
    "# print('Started loading merged_cycle_data_file.')\n",
    "# iter_ = pd.read_csv(merged_cycle_data_file, chunksize=chunk_size, iterator=True,\n",
    "#         index_col='Rental Id',\n",
    "#         parse_dates=['End Date', 'Start Date'])\n",
    "# print('Finished loading merged_cycle_data_file.')\n",
    "\n",
    "# r_start = time.time()\n",
    "# for i, df in enumerate(iter_):\n",
    "#     r_end = time.time()\n",
    "#     print(f'{i+1}. Read rows {chunk_size*i}:{chunk_size*(i+1)} in {r_end-r.start:.3f}. ', end='')\n",
    "\n",
    "#     start = time.time()\n",
    "#     df = df.dropna()\n",
    "#     # leave only entries that have valid duration\n",
    "#     df = df[df['Duration'] > 0]\n",
    "    \n",
    "#     diff = df['End Date'] - df['Start Date'] # compute the difference between the objects\n",
    "#     seconds = diff.map(lambda x: x.total_seconds()) # map to seconds\n",
    "#     df = df[(df['Duration'] == seconds) & (seconds >= 0)] # check if duration matches the result and if the result is positive\n",
    "    \n",
    "    \n",
    "#     # keep only year, month, day, hour information from the start date\n",
    "#     df['Start Date'] = df['Start Date'].map(date_mapper)\n",
    "    \n",
    "#     share_df = df.groupby('Start Date').agg({'Start Date': 'count'}).rename(columns={'Start Date': 'share_count'})\n",
    "#     share_df = share_df.join(weather_df)\n",
    "#     share_df = share_df.reset_index()\n",
    "#     share_df = share_df.dropna()\n",
    "    \n",
    "#     share_df['month'] = share_df['Start Date'].apply(lambda t: t.month)\n",
    "#     share_df['weekday'] = share_df['Start Date'].apply(lambda t: t.weekday())\n",
    "#     share_df['hour'] = share_df['Start Date'].apply(lambda t: t.hour)\n",
    "#     share_df['is_holiday'] = share_df['Start Date'].map(lambda x: x.date() in hol_set).map(lambda x: '1' if x else '0')\n",
    "#     # check if start date hits on a weekend\n",
    "#     # monday is 0, sunday is 6\n",
    "#     share_df['is_weekend'] = share_df['Start Date'].map(lambda x: x.weekday() > 4).map(lambda x: '1' if x else '0')\n",
    "#     share_df['weatherCode'] = share_df['weatherCode'].map(lambda x: str(int(x)))\n",
    "    \n",
    "#     bike_share_df = bike_share_df.append(share_df)\n",
    "#     end = time.time()\n",
    "#     print(f'Completed cleaning & merging in {end-start:3.3f} seconds.')\n",
    "#     r_start = time.time()\n",
    "\n",
    "# print('Finished reading!')\n",
    "# bike_share_df = bike_share_df.reset_index().drop(columns=['index']) # fix the index\n",
    "# # save the data to a file, so that we can load it faster next time\n",
    "# bike_share_df.to_csv('shares-ungrouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {\n",
    "    'weatherCode': str,\n",
    "    'is_holiday': str,\n",
    "    'is_weekend': str\n",
    "}\n",
    "bike_share_df = pd.read_csv('shares-ungrouped.csv', parse_dates=['Start Date'], dtype=types).drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because of reading in chunks, some hours appear multiple times\n",
    "bike_share_df[bike_share_df['Start Date'] == '2018-12-07 10:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_total = bike_share_df.groupby('Start Date').agg({'share_count': 'sum'})\n",
    "share_row = bike_share_df.groupby('Start Date').agg(lambda x: x.iloc[0])\n",
    "share_row['share_count'] = share_total['share_count']\n",
    "bike_share_df = share_row.reset_index()\n",
    "bike_share_df.to_csv('shares-grouped.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "bike_share_df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['month'].hist(bins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['weatherCode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['hour'].hist(bins=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['temperature'].hist(bins=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df[bike_share_df['share_count'] <= 100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['share_count'].hist(bins=2000, figsize=(24,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df[bike_share_df['share_count'] <= 300]['share_count'].hist(bins=100, figsize=(24,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelBinarizer can apply the transformation from text categories\n",
    "# to integer categories, then from integer categories to one-hot vectors\n",
    "# basically, it combines a label encoder with one-hot encoder\n",
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "# encoder = LabelBinarizer()\n",
    "# holiday_cat = bike_share_df['weatherCode']\n",
    "# holiday_cat_1hot = encoder.fit_transform(holiday_cat.to_numpy())\n",
    "# holiday_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_weather_codes = pd.get_dummies(bike_share_df['weatherCode'], prefix='weather')\n",
    "bike_share_df_enc = bike_share_df.drop(['weatherCode'], axis=1)\n",
    "bike_share_df_enc = bike_share_df_enc.join(encoded_weather_codes)\n",
    "bike_share_df_enc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_enc = pd.get_dummies(bike_share_df['is_holiday'], prefix='holiday')\n",
    "week_enc = pd.get_dummies(bike_share_df['is_weekend'], prefix='weekend')\n",
    "bike_share_df_enc = bike_share_df_enc.join(holiday_enc).join(week_enc)\n",
    "bike_share_df_enc = bike_share_df_enc.drop(['is_holiday', 'is_weekend'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"weekday\" could be dropped due to the fact that we have \"is_weekend\"\n",
    "# \"month\" could be dropped due to the fact that the model might not be able to extract any useful information\n",
    "# out of it\n",
    "bike_share_df_enc = bike_share_df_enc.drop(['weekday'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bike_share_df_enc['hr_sin'] = np.sin(bike_share_df_enc.hour*(2.*np.pi/24))\n",
    "bike_share_df_enc['hr_cos'] = np.cos(bike_share_df_enc.hour*(2.*np.pi/24))\n",
    "bike_share_df_enc['mnth_sin'] = np.sin((bike_share_df_enc.month-1)*(2.*np.pi/12))\n",
    "bike_share_df_enc['mnth_cos'] = np.cos((bike_share_df_enc.month-1)*(2.*np.pi/12))\n",
    "bike_share_df_enc = bike_share_df_enc.drop(['month', 'hour'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df_enc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import Lasso, ElasticNet, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(bike_share_df_enc.drop('Start Date', axis=1), test_size=0.2, random_state=42069)\n",
    "X_train = train_set.drop('share_count', axis=1).to_numpy()\n",
    "Y_train = train_set['share_count'].to_numpy()\n",
    "\n",
    "X_test = test_set.drop('share_count', axis=1).to_numpy()\n",
    "Y_test = test_set['share_count'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state =42069\n",
    "classifiers = [\n",
    "#     ('Decision Tree', DecisionTreeClassifier()),\n",
    "#     ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
    "#     ('NN (50, 50, 16)', MLPClassifier(hidden_layer_sizes=(16, 16, 16), verbose=True, learning_rate='adaptive', activation='tanh', learning_rate_init=0.01))\n",
    "    ('Lasso', Lasso(random_state=random_state)),\n",
    "    ('ElasticNet', ElasticNet(random_state=random_state)),\n",
    "    ('Ridge', Ridge(random_state=random_state)),\n",
    "    ('SVR liniar', SVR(kernel='linear', verbose=True)),\n",
    "    ('SVR rbf', SVR(kernel='rbf', verbose=True)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def learn(classifiers, X_train, Y_train, X_test, Y_test):\n",
    "    for name, clf in classifiers:\n",
    "        print(f'** {name}')\n",
    "        t0 = time.time()\n",
    "        clf.fit(X_train, Y_train)\n",
    "        t1 = time.time()\n",
    "        score_train = clf.score(X_train[:10000], Y_train[:10000])\n",
    "        t2 = time.time()\n",
    "        score_test = clf.score(X_test, Y_test)\n",
    "        t3 = time.time()\n",
    "        print(f'\\tTraining time {t1 - t0:3.3f}')\n",
    "        print(f'\\tPrediction time (train) {t2 - t1:3.3f}')\n",
    "        print(f'\\tPrediction time (test) {t3 - t2:3.3f}')\n",
    "        print(f'\\tScore train: {score_train:.3f}\\tScore Test: {score_test:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(classifiers, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate the classifier\n",
    "classifiers = [\n",
    "#     ('Decision Tree', DecisionTreeClassifier()),\n",
    "#     ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
    "#     ('NN (50, 50, 16)', MLPClassifier(hidden_layer_sizes=(16, 16, 16), verbose=True, learning_rate='adaptive', activation='tanh', learning_rate_init=0.01))\n",
    "    ('Lasso', Lasso(random_state=random_state)),\n",
    "    ('ElasticNet', ElasticNet(random_state=random_state)),\n",
    "    ('Ridge', Ridge(random_state=random_state)),\n",
    "    ('SVR liniar', SVR(kernel='linear', verbose=True)),\n",
    "    ('SVR rbf', SVR(kernel='rbf', verbose=True)),\n",
    "]\n",
    "learn(classifiers, X_train_scaled, Y_train, X_test_scaled, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp, Y_train_temp = train_set['temperature'].to_numpy().reshape(-1, 1), train_set['share_count']\n",
    "X_test_temp, Y_test_temp = test_set['temperature'].to_numpy().reshape(-1, 1), test_set['share_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
    "#     ('NN (50, 50, 16)', MLPClassifier(hidden_layer_sizes=(16, 16, 16), verbose=True, learning_rate='adaptive', activation='tanh', learning_rate_init=0.01))\n",
    "    ('Lasso', Lasso(random_state=random_state)),\n",
    "    ('ElasticNet', ElasticNet(random_state=random_state)),\n",
    "    ('Ridge', Ridge(random_state=random_state)),\n",
    "    ('SVR liniar', SVR(kernel='linear', verbose=True)),\n",
    "    ('SVR rbf', SVR(kernel='rbf', verbose=True)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(classifiers, X_train_temp, Y_train_temp, X_test_temp, Y_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noBSdata = bike_share_df_enc.drop(['Start Date', 'hr_sin', 'hr_cos', 'mnth_sin', 'mnth_cos', 'holiday_0', 'holiday_1', 'weekend_1', 'weekend_0'], axis = 1)\n",
    "\n",
    "train_set, test_set = train_test_split(noBSdata, test_size=0.2, random_state=42069)\n",
    "X_train = train_set.drop('share_count', axis=1).to_numpy()\n",
    "Y_train = train_set['share_count'].to_numpy()\n",
    "\n",
    "X_test = test_set.drop('share_count', axis=1).to_numpy()\n",
    "Y_test = test_set['share_count'].to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "classifiers = [\n",
    "#     ('Decision Tree', DecisionTreeClassifier()),\n",
    "#     ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
    "#     ('NN (50, 50, 16)', MLPClassifier(hidden_layer_sizes=(16, 16, 16), verbose=True, learning_rate='adaptive', activation='tanh', learning_rate_init=0.01))\n",
    "    ('Lasso', Lasso(random_state=random_state)),\n",
    "    ('ElasticNet', ElasticNet(random_state=random_state)),\n",
    "    ('Ridge', Ridge(random_state=random_state)),\n",
    "    ('SVR liniar', SVR(kernel='linear', verbose=True)),\n",
    "    ('SVR rbf', SVR(kernel='rbf', verbose=True)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(classifiers, X_train_scaled, Y_train, X_test_scaled, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import main\n",
    "key = '<replace with your key>' # this is the key for the weather data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this line on your own risk\n",
    "# it will take ~15-20 minutes to download the data and to do some cleaning\n",
    "#main(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import merged_cycle_data_file\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the weather & holiday data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import weather_data_csv\n",
    "\n",
    "holidays = 'holidays.csv'\n",
    "hol_df = pd.read_csv(holidays, index_col='date', parse_dates=['date'])\n",
    "weather_df = pd.read_csv(weather_data_csv, index_col='timestamp', parse_dates=['timestamp'])\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a set of the holiday dates\n",
    "hol_set= set(hol_df.index.map(lambda x: x.date()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the merged csv file by reading it in chunks\n",
    "\n",
    "# Warning:\n",
    "The next cell takes a lot of time (on one machine it took 3h 40 mins) so skip running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from datetime import datetime\n",
    "# import time\n",
    "# chunk_size = 1000000\n",
    "# date_mapper = lambda x: pd.to_datetime(datetime(year=x.year, month=x.month, day=x.day, hour=x.hour))\n",
    "\n",
    "# bike_share_df = pd.DataFrame()\n",
    "# print('Started loading merged_cycle_data_file.')\n",
    "# iter_ = pd.read_csv(merged_cycle_data_file, chunksize=chunk_size, iterator=True,\n",
    "#         index_col='Rental Id',\n",
    "#         parse_dates=['End Date', 'Start Date'])\n",
    "# print('Finished loading merged_cycle_data_file.')\n",
    "\n",
    "# r_start = time.time()\n",
    "# for i, df in enumerate(iter_):\n",
    "#     r_end = time.time()\n",
    "#     print(f'{i+1}. Read rows {chunk_size*i}:{chunk_size*(i+1)} in {r_end-r.start:.3f}. ', end='')\n",
    "\n",
    "#     start = time.time()\n",
    "#     df = df.dropna()\n",
    "#     # leave only entries that have valid duration\n",
    "#     df = df[df['Duration'] > 0]\n",
    "    \n",
    "#     diff = df['End Date'] - df['Start Date'] # compute the difference between the objects\n",
    "#     seconds = diff.map(lambda x: x.total_seconds()) # map to seconds\n",
    "#     df = df[(df['Duration'] == seconds) & (seconds >= 0)] # check if duration matches the result and if the result is positive\n",
    "    \n",
    "    \n",
    "#     # keep only year, month, day, hour information from the start date\n",
    "#     df['Start Date'] = df['Start Date'].map(date_mapper)\n",
    "    \n",
    "#     share_df = df.groupby('Start Date').agg({'Start Date': 'count'}).rename(columns={'Start Date': 'share_count'})\n",
    "#     share_df = share_df.join(weather_df)\n",
    "#     share_df = share_df.reset_index()\n",
    "#     share_df = share_df.dropna()\n",
    "    \n",
    "#     share_df['month'] = share_df['Start Date'].apply(lambda t: t.month)\n",
    "#     share_df['weekday'] = share_df['Start Date'].apply(lambda t: t.weekday())\n",
    "#     share_df['hour'] = share_df['Start Date'].apply(lambda t: t.hour)\n",
    "#     share_df['is_holiday'] = share_df['Start Date'].map(lambda x: x.date() in hol_set).map(lambda x: '1' if x else '0')\n",
    "#     # check if start date hits on a weekend\n",
    "#     # monday is 0, sunday is 6\n",
    "#     share_df['is_weekend'] = share_df['Start Date'].map(lambda x: x.weekday() > 4).map(lambda x: '1' if x else '0')\n",
    "#     share_df['weatherCode'] = share_df['weatherCode'].map(lambda x: str(int(x)))\n",
    "    \n",
    "#     bike_share_df = bike_share_df.append(share_df)\n",
    "#     end = time.time()\n",
    "#     print(f'Completed cleaning & merging in {end-start:3.3f} seconds.')\n",
    "#     r_start = time.time()\n",
    "\n",
    "# print('Finished reading!')\n",
    "# bike_share_df = bike_share_df.reset_index().drop(columns=['index']) # fix the index\n",
    "# # save the data to a file, so that we can load it faster next time\n",
    "# bike_share_df.to_csv('shares-ungrouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {\n",
    "    'weatherCode': str,\n",
    "    'is_holiday': str,\n",
    "    'is_weekend': str\n",
    "}\n",
    "bike_share_df = pd.read_csv('shares-ungrouped.csv', parse_dates=['Start Date'], dtype=types).drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df[bike_share_df['Start Date'] == '2018-01-01 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because of reading in chunks, some hours appear multiple times\n",
    "bike_share_df[bike_share_df['Start Date'] == '2018-12-07 10:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_total = bike_share_df.groupby('Start Date').agg({'share_count': 'sum'})\n",
    "share_row = bike_share_df.groupby('Start Date').agg(lambda x: x.iloc[0])\n",
    "share_row['share_count'] = share_total['share_count']\n",
    "bike_share_df = share_row.reset_index()\n",
    "bike_share_df.to_csv('shares-grouped.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "bike_share_df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['month'].hist(bins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['weatherCode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['hour'].hist(bins=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['temperature'].hist(bins=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df[bike_share_df['share_count'] <= 100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['share_count'].hist(bins=2000, figsize=(24,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df[bike_share_df['share_count'] <= 300]['share_count'].hist(bins=100, figsize=(24,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = bike_share_df.copy()\n",
    "\n",
    "# NEW_CODE: [WEATHER_CODES] (GROUP_NAME)\n",
    "code_table = {\n",
    "    1: [113, 113], # (\"Clear\")\n",
    "    2: [116, 122, 119], # (\"Cloudy\")\n",
    "    3: [266, 263, 176, 353, 296, 293], # (\"Light rain\")\n",
    "    4: [302, 299, 256, 308, 359, 305], # (\"Heavy rain\")\n",
    "    5: [317, 311, 326, 362, 326, 368], # (\"Light snow\")\n",
    "    6: [228, 227, 371, 332, 320, 335, 329], # (\"Heavy snow\")\n",
    "    7: [143, 248], # (\"Lowered visibility\")\n",
    "    8: [200, 389, 386], # (\"Thunder\")\n",
    "    9: [230] # (\"Blizzard\")\n",
    "}\n",
    "\n",
    "def mapWeatherCode(old_weather_code):\n",
    "    for new_code in code_table:\n",
    "        if (int(old_weather_code) in code_table[new_code]):\n",
    "            return str(new_code)\n",
    "\n",
    "test_df['weatherCode'] = test_df['weatherCode'].map(lambda x: mapWeatherCode(x))\n",
    "\n",
    "bike_share_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['weatherCode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelBinarizer can apply the transformation from text categories\n",
    "# to integer categories, then from integer categories to one-hot vectors\n",
    "# basically, it combines a label encoder with one-hot encoder\n",
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "# encoder = LabelBinarizer()\n",
    "# holiday_cat = bike_share_df['weatherCode']\n",
    "# holiday_cat_1hot = encoder.fit_transform(holiday_cat.to_numpy())\n",
    "# holiday_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = bike_share_df.copy()\n",
    "\n",
    "tmp['lag_1'] = tmp['share_count'].shift(1)\n",
    "tmp['lag_2'] = tmp['share_count'].shift(2)\n",
    "tmp['lag_3'] = tmp['share_count'].shift(3)\n",
    "\n",
    "tmp = tmp.drop([0, 1, 2])\n",
    "\n",
    "bike_share_df = tmp.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_weather_codes = pd.get_dummies(bike_share_df['weatherCode'], prefix='weather')\n",
    "bike_share_df_enc = bike_share_df.drop(['weatherCode'], axis=1)\n",
    "bike_share_df_enc = bike_share_df_enc.join(encoded_weather_codes)\n",
    "bike_share_df_enc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_enc = pd.get_dummies(bike_share_df['is_holiday'], prefix='holiday')\n",
    "week_enc = pd.get_dummies(bike_share_df['is_weekend'], prefix='weekend')\n",
    "bike_share_df_enc = bike_share_df_enc.join(holiday_enc).join(week_enc)\n",
    "bike_share_df_enc = bike_share_df_enc.drop(['is_holiday', 'is_weekend'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"weekday\" could be dropped due to the fact that we have \"is_weekend\"\n",
    "# \"month\" could be dropped due to the fact that the model might not be able to extract any useful information\n",
    "# out of it\n",
    "bike_share_df_enc = bike_share_df_enc.drop(['weekday'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bike_share_df_enc['hr_sin'] = np.sin(bike_share_df_enc.hour*(2.*np.pi/24))\n",
    "bike_share_df_enc['hr_cos'] = np.cos(bike_share_df_enc.hour*(2.*np.pi/24))\n",
    "bike_share_df_enc['mnth_sin'] = np.sin((bike_share_df_enc.month-1)*(2.*np.pi/12))\n",
    "bike_share_df_enc['mnth_cos'] = np.cos((bike_share_df_enc.month-1)*(2.*np.pi/12))\n",
    "bike_share_df_enc = bike_share_df_enc.drop(['month', 'hour'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df_enc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import Lasso, ElasticNet, Ridge, SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.4\n",
    "train_set, test_set = train_test_split(bike_share_df_enc.drop('Start Date', axis=1), test_size=test_size, random_state=42069)\n",
    "X_train = train_set.drop('share_count', axis=1).to_numpy()\n",
    "Y_train = train_set['share_count'].to_numpy()\n",
    "\n",
    "X_test = test_set.drop('share_count', axis=1).to_numpy()\n",
    "Y_test = test_set['share_count'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42069\n",
    "classifiers = [\n",
    "#     ('Decision Tree', DecisionTreeClassifier()),\n",
    "#     ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
    "#     ('NN (50, 50, 16)', MLPClassifier(hidden_layer_sizes=(16, 16, 16), verbose=True, learning_rate='adaptive', activation='tanh', learning_rate_init=0.01))\n",
    "    ('Lasso', Lasso(random_state=random_state)),\n",
    "    ('ElasticNet', ElasticNet(random_state=random_state)),\n",
    "    ('Ridge', Ridge(random_state=random_state)),\n",
    "#     ('SVR linear', SVR(kernel='linear', verbose=True)),\n",
    "#     ('SVR rbf', SVR(kernel='rbf', verbose=True)),\n",
    "#     ('SGDRegressor', SGDRegressor()),\n",
    "    ('GradientBoostRegressor', GradientBoostingRegressor()),\n",
    "    ('GradientBoostRegressor(ls)', GradientBoostingRegressor(max_features=21, random_state=random_state, max_depth=8)),\n",
    "    ('Optimized GradientBoostRegressor(huber)', GradientBoostingRegressor(loss='huber', max_features=21, n_estimators=160, random_state=random_state, max_depth=8))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def learn(classifiers, X_train, Y_train, X_test, Y_test):\n",
    "    for name, clf in classifiers:\n",
    "        print(f'** {name}')\n",
    "        t0 = time.time()\n",
    "        clf.fit(X_train, Y_train)\n",
    "        t1 = time.time()\n",
    "        score_train = clf.score(X_train[:10000], Y_train[:10000])\n",
    "        t2 = time.time()\n",
    "        score_test = clf.score(X_test, Y_test)\n",
    "        t3 = time.time()\n",
    "        print(f'\\tTraining time {t1 - t0:3.3f}')\n",
    "        print(f'\\tPrediction time (train) {t2 - t1:3.3f}')\n",
    "        print(f'\\tPrediction time (test) {t3 - t2:3.3f}')\n",
    "        print(f'\\tScore train: {score_train:.3f}\\tScore Test: {score_test:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn(classifiers, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Search through param_grid parameters, find the best set of parameters for GBR (Gradient Boosting Regressor)\n",
    "# param_grid = [\n",
    "#     {'max_features': [21], 'max_depth': [3, 10, 20, 50, 100]}\n",
    "#   ]\n",
    "\n",
    "# clf = GradientBoostingRegressor(random_state=random_state)\n",
    "\n",
    "# grid_search = GridSearchCV(clf, param_grid, cv=5,\n",
    "#                            scoring='neg_mean_squared_error', return_train_score=True)\n",
    "# grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# # Run GBR with best set of parameters\n",
    "# clf = GradientBoostingRegressor(random_state=random_state,\n",
    "#                                 **grid_search.best_params_,\n",
    "#                                 n_estimators=2000)\n",
    "\n",
    "# clf.fit(X_train, Y_train)\n",
    "# results = clf.predict(X_test)\n",
    "# score = clf.score(X_test, Y_test)\n",
    "# print(\"TEST SCORE: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_clf = GradientBoostingRegressor(loss='huber', max_features=23, n_estimators=160, random_state=random_state, max_depth=8)\n",
    "optimized_clf.fit(X_train, Y_train)\n",
    "results = optimized_clf.predict(X_test)\n",
    "\n",
    "score = optimized_clf.score(X_test, Y_test)\n",
    "print(\"Test score: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_mse = mean_squared_error(Y_test, results)\n",
    "lin_rmse = np.sqrt(lin_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lin_mae = mean_absolute_error(Y_test, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, match in enumerate(results < 0):\n",
    "#     if match:\n",
    "#         print(bike_share_df.iloc[index], end='\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_negative = len([i for i in results if i < 0]) \n",
    "count_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "y_true = Y_test\n",
    "y_pred = results\n",
    "var_score = explained_variance_score(y_true, y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean squared error: \" + str(lin_rmse))\n",
    "print(\"Mean squared error: \" + str(lin_mae))\n",
    "print(\"Variance score: \" + str(var_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import main\n",
    "key = '<replace with your key>' # this is the key for the weather data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this line on your own risk\n",
    "# it will take ~15-20 minutes to download the data and to do some cleaning\n",
    "# main(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import merged_cycle_data_file\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the weather & holiday data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import weather_data_csv\n",
    "\n",
    "holidays = 'holidays.csv'\n",
    "hol_df = pd.read_csv(holidays, index_col='date', parse_dates=['date'])\n",
    "weather_df = pd.read_csv(weather_data_csv, index_col='timestamp', parse_dates=['timestamp'])\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a set of the holiday dates\n",
    "hol_set= set(hol_df.index.map(lambda x: x.date()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the merged csv file by reading it in chunks\n",
    "\n",
    "# Warning:\n",
    "The next cell takes a lot of time (it took 5h 10 mins for us) so uncomment this cell on your own risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from datetime import datetime\n",
    "# import time\n",
    "\n",
    "# chunk_size = 500000\n",
    "# date_mapper = lambda x: pd.to_datetime(datetime(year=x.year, month=x.month, day=x.day, hour=x.hour))\n",
    "\n",
    "# bike_share_df = pd.DataFrame()\n",
    "# print('Started loading merged_cycle_data_file.')\n",
    "# iter_ = pd.read_csv(merged_cycle_data_file, chunksize=chunk_size, iterator=True,\n",
    "#         index_col='Rental Id',\n",
    "#         parse_dates=['End Date', 'Start Date'])\n",
    "# print('Finished loading merged_cycle_data_file.')\n",
    "\n",
    "# r_start = time.time()\n",
    "# for i, df in enumerate(iter_):\n",
    "#     r_end = time.time()\n",
    "#     print(f'{i+1}. Read rows {chunk_size*i}:{chunk_size*(i+1)} in {r_end-r_start:.3f}. ', end='')\n",
    "\n",
    "#     start = time.time()\n",
    "#     df = df.dropna()\n",
    "#     # leave only entries that have valid duration\n",
    "#     df = df[df['Duration'] > 0]\n",
    "    \n",
    "#     diff = df['End Date'] - df['Start Date'] # compute the difference between the objects\n",
    "#     seconds = diff.map(lambda x: x.total_seconds()) # map to seconds\n",
    "#     df = df[(df['Duration'] == seconds) & (seconds >= 0)] # check if duration matches the result and if the result is positive\n",
    "    \n",
    "    \n",
    "#     # keep only year, month, day, hour information from the start date\n",
    "#     df['Start Date'] = df['Start Date'].map(date_mapper)\n",
    "#     # count the number of shares per hour\n",
    "#     share_df = df.groupby('Start Date').agg({'Start Date': 'count'}).rename(columns={'Start Date': 'share_count'})\n",
    "#     # join of weather data\n",
    "#     share_df = share_df.join(weather_df)\n",
    "#     # reset index\n",
    "#     share_df = share_df.reset_index().rename(columns={'index': 'Start Date'})\n",
    "#     share_df = share_df.dropna()\n",
    "    \n",
    "#     share_df['month'] = share_df['Start Date'].apply(lambda t: t.month)\n",
    "#     share_df['weekday'] = share_df['Start Date'].apply(lambda t: t.weekday())\n",
    "#     share_df['hour'] = share_df['Start Date'].apply(lambda t: t.hour)\n",
    "#     share_df['is_holiday'] = share_df['Start Date'].map(lambda x: x.date() in hol_set).map(lambda x: '1' if x else '0')\n",
    "#     # check if start date hits on a weekend\n",
    "#     # monday is 0, sunday is 6\n",
    "#     share_df['is_weekend'] = share_df['Start Date'].map(lambda x: x.weekday() > 4).map(lambda x: '1' if x else '0')\n",
    "#     share_df['weatherCode'] = share_df['weatherCode'].map(lambda x: str(int(x)))\n",
    "    \n",
    "#     bike_share_df = bike_share_df.append(share_df)\n",
    "#     end = time.time()\n",
    "#     print(f'Completed cleaning & merging in {end-start:3.3f} seconds.')\n",
    "#     r_start = time.time()\n",
    "\n",
    "# print('Finished reading!')\n",
    "# # bike_share_df = bike_share_df.reset_index(drop=True)\n",
    "# # because of reading in chunks, some hours appear multiple times\n",
    "# print('Grouping the hours')\n",
    "# share_total = bike_share_df.groupby('Start Date').agg({'share_count': 'sum'})\n",
    "# share_row = bike_share_df.groupby('Start Date').agg(lambda x: x.iloc[0])\n",
    "# share_row['share_count'] = share_total['share_count']\n",
    "# bike_share_df = share_row.reset_index()\n",
    "# print('Saving to file')\n",
    "# # save the data to a file, so that we can load it faster next time\n",
    "# bike_share_df.sort_values('Start Date').reset_index(drop=True).to_csv('bike-shares.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {\n",
    "    'weatherCode': str,\n",
    "    'is_holiday': str,\n",
    "    'is_weekend': str\n",
    "}\n",
    "bike_share_df = pd.read_csv('bike-shares.csv', parse_dates=['Start Date'], dtype=types)\n",
    "fig_size = (18, 25)\n",
    "random_state = 42069\n",
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df, test_set = train_test_split(bike_share_df, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "bike_share_df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['month'].hist(bins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['weatherCode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['hour'].hist(bins=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['temperature'].hist(bins=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df[bike_share_df['share_count'] <= 100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df['share_count'].hist(bins=200, figsize=(24,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df[bike_share_df['share_count'] <= 300]['share_count'].hist(bins=100, figsize=(24,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEATHER_CODE: [GROUP_NAME]\n",
    "new_codes = {\n",
    "    '113': 'Clear', # Clear, Sunny\n",
    "    '116': 'Cloudy', # Partly cloudy\n",
    "    '122': 'Cloudy', # Overcast\n",
    "    '119': 'Cloudy', # Cloudy\n",
    "    '143': 'Lowered visibility', # Mist\n",
    "    '176': 'Light rain', # Patchy rain possible\n",
    "    '296': 'Light rain', # Light rain\n",
    "    '308': 'Heavy rain', # Heavy rain\n",
    "    '302': 'Heavy rain', # Moderate rain\n",
    "    '353': 'Light rain', # Light rain shower\n",
    "    '266': 'Light rain', # Light drizzle\n",
    "    '356': 'Heavy rain', # Moderate or heavy rain shower\n",
    "    '293': 'Light rain', # Patchy light rain\n",
    "    '248': 'Lowered visibility', # Fog\n",
    "    '200': 'Thunder', # Thundery outbreaks possible\n",
    "    '299': 'Heavy rain', # Moderate rain at times\n",
    "    '263': 'Light rain', # Patchy light drizzle\n",
    "    '317': 'Light snow', # Light sleet\n",
    "    '386': 'Thunder', # Patchy light rain with thunder\n",
    "    '230': 'Blizzard', # Blizzard\n",
    "    '329': 'Heavy snow', # Patchy moderate snow\n",
    "    '332': 'Heavy snow', # Moderate snow\n",
    "    '338': 'Heavy snow', # Heavy snow\n",
    "    '326': 'Light snow', # Light snow\n",
    "    '362': 'Light snow', # Light sleet showers\n",
    "    '389': 'Thunder', # Moderate or heavy rain with thunder\n",
    "    '311': 'Light snow', # Light freezing rain\n",
    "    '359': 'Heavy rain', # Torrential rain shower\n",
    "    '323': 'Light snow', # Patchy light snow\n",
    "    '260': 'Lowered visibility', # Freezing fog\n",
    "    '368': 'Light snow', # Light snow showers\n",
    "    '371': 'Heavy snow', # Moderate or heavy snow showers\n",
    "    '305': 'Heavy rain', # Heavy rain at times\n",
    "    '320': 'Heavy snow', # Moderate or heavy sleet\n",
    "    '335': 'Heavy snow', # Patchy heavy snow\n",
    "    '227': 'Heavy snow', # Blowing snow\n",
    "}\n",
    "\n",
    "bike_share_df['weatherCode'] = bike_share_df['weatherCode'].map(lambda x: new_codes[x])\n",
    "bike_share_df['weatherCode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1, ax2, ax3)= plt.subplots(nrows=3)\n",
    "fig.set_size_inches(fig_size)\n",
    "\n",
    "sns.pointplot(data=bike_share_df, x='weatherCode', y='share_count', ax=ax1, join=False)\n",
    "sns.pointplot(data=bike_share_df, x='hour', y='share_count', hue='is_holiday', ax=ax2)\n",
    "sns.pointplot(data=bike_share_df, x='hour', y='share_count', hue='is_weekend', ax=ax3)\n",
    "ax1.set_xlabel('Weather code', labelpad=30)\n",
    "_ = ax1.set_ylabel('Share count', labelpad=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with cyclical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bike_share_df['hr_sin'] = np.sin(bike_share_df.hour*(2.*np.pi/24))\n",
    "bike_share_df['hr_cos'] = np.cos(bike_share_df.hour*(2.*np.pi/24))\n",
    "bike_share_df['mnth_sin'] = np.sin((bike_share_df.month-1)*(2.*np.pi/12))\n",
    "bike_share_df['mnth_cos'] = np.cos((bike_share_df.month-1)*(2.*np.pi/12))\n",
    "bike_share_df = bike_share_df.drop(['month', 'hour'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(nrows=1)\n",
    "fig.set_size_inches(15,10)\n",
    "\n",
    "pearsoncorr = bike_share_df.drop('Start Date', axis=1).corr(method='pearson')\n",
    "heat_map=sns.heatmap(pearsoncorr, \n",
    "            xticklabels=pearsoncorr.columns,\n",
    "            yticklabels=pearsoncorr.columns,\n",
    "            cmap='RdBu_r',\n",
    "            annot=True,\n",
    "            linewidth=0.5,\n",
    "           ax=ax)\n",
    "_ = heat_map.set_yticklabels(heat_map.get_yticklabels(), rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df_lag = bike_share_df.sort_values('Start Date')\n",
    "\n",
    "bike_share_df_lag['lag_1'] = bike_share_df_lag['share_count'].shift(1)\n",
    "bike_share_df_lag['lag_2'] = bike_share_df_lag['share_count'].shift(2)\n",
    "bike_share_df_lag['lag_3'] = bike_share_df_lag['share_count'].shift(3)\n",
    "\n",
    "bike_share_df_lag = bike_share_df_lag[3:].reset_index(drop=True)\n",
    "\n",
    "bike_share_df_lag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(nrows=1)\n",
    "fig.set_size_inches(15,10)\n",
    "\n",
    "pearsoncorr = bike_share_df_lag.drop('Start Date', axis=1).corr(method='pearson')\n",
    "heat_map=sns.heatmap(pearsoncorr, \n",
    "            xticklabels=pearsoncorr.columns,\n",
    "            yticklabels=pearsoncorr.columns,\n",
    "            cmap='RdBu_r',\n",
    "            annot=True,\n",
    "            linewidth=0.5,\n",
    "           ax=ax)\n",
    "_ = heat_map.set_yticklabels(heat_map.get_yticklabels(), rotation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_weather_codes = pd.get_dummies(bike_share_df['weatherCode'], prefix='weather')\n",
    "bike_share_df_enc = bike_share_df.drop(['weatherCode'], axis=1)\n",
    "bike_share_df_enc = bike_share_df_enc.join(encoded_weather_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_enc = pd.get_dummies(bike_share_df['is_holiday'], prefix='holiday')\n",
    "week_enc = pd.get_dummies(bike_share_df['is_weekend'], prefix='weekend')\n",
    "bike_share_df_enc = bike_share_df_enc.join(holiday_enc).join(week_enc)\n",
    "bike_share_df_enc = bike_share_df_enc.drop(['is_holiday', 'is_weekend'], axis=1)\n",
    "# \"weekday\" could be dropped due to the fact that we have \"is_weekend\"\n",
    "bike_share_df_enc = bike_share_df_enc.drop(['weekday'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(nrows=1)\n",
    "fig.set_size_inches(fig_size)\n",
    "\n",
    "pearsoncorr = bike_share_df_enc.drop('Start Date', axis=1).corr(method='pearson')\n",
    "heat_map=sns.heatmap(pearsoncorr, \n",
    "            xticklabels=pearsoncorr.columns,\n",
    "            yticklabels=pearsoncorr.columns,\n",
    "            cmap='RdBu_r',\n",
    "            annot=True,\n",
    "            linewidth=0.5,\n",
    "           ax=ax)\n",
    "_ = heat_map.set_yticklabels(heat_map.get_yticklabels(), rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import Lasso, ElasticNet, Ridge, SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = bike_share_df_enc.drop('Start Date', axis=1)\n",
    "test_set = test_set.drop('Start Date', axis=1)\n",
    "\n",
    "X_train = train_set.drop('share_count', axis=1).to_numpy()\n",
    "Y_train = train_set['share_count'].to_numpy()\n",
    "\n",
    "X_test = test_set.drop('share_count', axis=1).to_numpy()\n",
    "Y_test = test_set['share_count'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "#     ('Decision Tree', DecisionTreeClassifier()),\n",
    "#     ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
    "#     ('NN (50, 50, 16)', MLPClassifier(hidden_layer_sizes=(16, 16, 16), verbose=True, learning_rate='adaptive', activation='tanh', learning_rate_init=0.01))\n",
    "    ('Lasso', Lasso(random_state=random_state)),\n",
    "    ('ElasticNet', ElasticNet(random_state=random_state)),\n",
    "    ('Ridge', Ridge(random_state=random_state)),\n",
    "#     ('SVR linear', SVR(kernel='linear', verbose=True)),\n",
    "#     ('SVR rbf', SVR(kernel='rbf', verbose=True)),\n",
    "#     ('SGDRegressor', SGDRegressor()),\n",
    "    ('GradientBoostRegressor', GradientBoostingRegressor()),\n",
    "    ('GradientBoostRegressor(ls)', GradientBoostingRegressor(max_features=21, random_state=random_state, max_depth=8)),\n",
    "    ('Optimized GradientBoostRegressor(huber)', GradientBoostingRegressor(loss='huber', max_features=21, n_estimators=160, random_state=random_state, max_depth=8))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def learn(classifiers, X_train, Y_train, X_test, Y_test):\n",
    "    for name, clf in classifiers:\n",
    "        print(f'** {name}')\n",
    "        t0 = time.time()\n",
    "        clf.fit(X_train, Y_train)\n",
    "        t1 = time.time()\n",
    "        score_train = clf.score(X_train[:10000], Y_train[:10000])\n",
    "        t2 = time.time()\n",
    "        score_test = clf.score(X_test, Y_test)\n",
    "        t3 = time.time()\n",
    "        print(f'\\tTraining time {t1 - t0:3.3f}')\n",
    "        print(f'\\tPrediction time (train) {t2 - t1:3.3f}')\n",
    "        print(f'\\tPrediction time (test) {t3 - t2:3.3f}')\n",
    "        print(f'\\tScore train: {score_train:.3f}\\tScore Test: {score_test:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn(classifiers, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Search through param_grid parameters, find the best set of parameters for GBR (Gradient Boosting Regressor)\n",
    "# param_grid = [\n",
    "#     {'max_features': [21], 'max_depth': [3, 10, 20, 50, 100]}\n",
    "#   ]\n",
    "\n",
    "# clf = GradientBoostingRegressor(random_state=random_state)\n",
    "\n",
    "# grid_search = GridSearchCV(clf, param_grid, cv=5,\n",
    "#                            scoring='neg_mean_squared_error', return_train_score=True)\n",
    "# grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# # Run GBR with best set of parameters\n",
    "# clf = GradientBoostingRegressor(random_state=random_state,\n",
    "#                                 **grid_search.best_params_,\n",
    "#                                 n_estimators=2000)\n",
    "\n",
    "# clf.fit(X_train, Y_train)\n",
    "# results = clf.predict(X_test)\n",
    "# score = clf.score(X_test, Y_test)\n",
    "# print(\"TEST SCORE: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_clf = GradientBoostingRegressor(loss='huber', max_features=23, n_estimators=160, random_state=random_state, max_depth=8)\n",
    "optimized_clf.fit(X_train, Y_train)\n",
    "results = optimized_clf.predict(X_test)\n",
    "\n",
    "score = optimized_clf.score(X_test, Y_test)\n",
    "print(\"Test score: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_mse = mean_squared_error(Y_test, results)\n",
    "lin_rmse = np.sqrt(lin_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lin_mae = mean_absolute_error(Y_test, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count negative\n",
    "(results < 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "y_true = Y_test\n",
    "y_pred = results\n",
    "var_score = explained_variance_score(y_true, y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean squared error: \" + str(lin_rmse))\n",
    "print(\"Mean squared error: \" + str(lin_mae))\n",
    "print(\"Variance score: \" + str(var_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
